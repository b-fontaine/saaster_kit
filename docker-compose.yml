services:
  # Gateway & WAF
  traefik:
    image: traefik:v2.10.4
    container_name: traefik
    restart: unless-stopped
    command:
      - "--api.insecure=true"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.traefik.address=:8080"
      - "--providers.file.directory=/etc/traefik/dynamic"
      - "--providers.file.watch=true"
    ports:
      - "80:80"
      - "443:443"
      - "8090:8080"  # Dashboard
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - ./infra/traefik/config:/etc/traefik
      - ./infra/traefik/dynamic:/etc/traefik/dynamic
    networks:
      - saaster-network
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.traefik.rule=Host(`traefik.localhost`)"
      - "traefik.http.routers.traefik.service=api@internal"
      - "traefik.http.routers.traefik.entrypoints=traefik"

  # Using Traefik's built-in ModSecurity middleware instead of a separate container

  kong:
    image: kong:3.3
    container_name: kong
    restart: unless-stopped
    environment:
      KONG_DATABASE: "off"
      KONG_DECLARATIVE_CONFIG: /etc/kong/kong.yml
      KONG_PROXY_ACCESS_LOG: /dev/stdout
      KONG_ADMIN_ACCESS_LOG: /dev/stdout
      KONG_PROXY_ERROR_LOG: /dev/stderr
      KONG_ADMIN_ERROR_LOG: /dev/stderr
      KONG_ADMIN_LISTEN: 0.0.0.0:8001
    volumes:
      - ./infra/kong/kong.yml:/etc/kong/kong.yml
    networks:
      - saaster-network
    expose:
      - 8000  # Proxy
      - 8001  # Admin API
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.kong.rule=Host(`api.localhost`)"
      - "traefik.http.services.kong.loadbalancer.server.port=8000"
      - "traefik.http.routers.kong.middlewares=kong-waf@file"
      - "traefik.http.routers.kong.entrypoints=websecure"
    depends_on:
      keycloak:
        condition: service_started
      temporal:
        condition: service_healthy

  # Observability
  elasticsearch:
    container_name: saaster-elasticsearch
    environment:
      - cluster.routing.allocation.disk.threshold_enabled=true
      - cluster.routing.allocation.disk.watermark.low=512mb
      - cluster.routing.allocation.disk.watermark.high=256mb
      - cluster.routing.allocation.disk.watermark.flood_stage=128mb
      - discovery.type=single-node
      - ES_JAVA_OPTS=-Xms256m -Xmx256m
      - xpack.security.enabled=false
    image: elasticsearch:${ELASTICSEARCH_VERSION}
    networks:
      - saaster-network
    expose:
      - 9200
    ports:
      - "9200:9200"
    volumes:
      - elasticsearch_data:/var/lib/elasticsearch/data
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:9200/_cluster/health"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  prometheus:
    image: prom/prometheus:v2.45.0
    container_name: prometheus
    volumes:
      - ./infra/prometheus/prometheus.yml:/etc/prometheus/prometheus.yml
      - prometheus_data:/prometheus
    command:
      - '--config.file=/etc/prometheus/prometheus.yml'
      - '--storage.tsdb.path=/prometheus'
      - '--web.console.libraries=/etc/prometheus/console_libraries'
      - '--web.console.templates=/etc/prometheus/consoles'
      - '--web.enable-lifecycle'
    ports:
      - "9090:9090"
    networks:
      - saaster-network
    restart: unless-stopped
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://localhost:9090/-/healthy"]
      interval: 10s
      timeout: 5s
      retries: 3

  grafana:
    image: grafana/grafana:10.0.3
    container_name: grafana
    volumes:
      - ./infra/grafana/provisioning:/etc/grafana/provisioning
      - grafana_data:/var/lib/grafana
    environment:
      - GF_SECURITY_ADMIN_USER=admin
      - GF_SECURITY_ADMIN_PASSWORD=admin
      - GF_USERS_ALLOW_SIGN_UP=false
    ports:
      - "3000:3000"
    networks:
      - saaster-network
    depends_on:
      - prometheus
      - elasticsearch
    restart: unless-stopped

  # Orchestration
  temporal-postgresql:
    container_name: temporal-postgresql
    restart: unless-stopped
    environment:
      POSTGRES_DB: temporal
      POSTGRES_PASSWORD: temporal
      POSTGRES_USER: temporal
    image: postgres:${POSTGRESQL_VERSION}
    networks:
      - temporal-network
      - saaster-network
    expose:
      - 5432
    volumes:
      - temporal_postgres_data:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U temporal -d temporal"]
      interval: 5s
      timeout: 5s
      retries: 5
  temporal:
    container_name: temporal
    depends_on:
      temporal-postgresql:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
    environment:
      - DB=postgres12
      - DB_PORT=5432
      - POSTGRES_USER=temporal
      - POSTGRES_PWD=temporal
      - POSTGRES_DB=temporal
      - POSTGRES_SEEDS=temporal-postgresql
      - DYNAMIC_CONFIG_FILE_PATH=config/dynamicconfig/docker.yaml
      - ENABLE_ES=true
      - ES_SEEDS=elasticsearch
      - ES_VERSION=v7
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CLI_ADDRESS=temporal:7233
      - SERVICES=history,matching,frontend,worker
      - LOG_LEVEL=info
    image: temporalio/auto-setup:${TEMPORAL_VERSION}
    networks:
      - saaster-network
      - temporal-network
    ports:
      - "7233:7233"
    volumes:
      - ./infra/temporal/dynamicconfig:/etc/temporal/config/dynamicconfig
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:7233/health || exit 0"]
      interval: 30s
      timeout: 10s
      retries: 15
      start_period: 60s
  temporal-admin-tools:
    container_name: temporal-admin-tools
    depends_on:
      temporal:
        condition: service_healthy
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CLI_ADDRESS=temporal:7233
    image: temporalio/admin-tools:${TEMPORAL_ADMINTOOLS_VERSION}
    networks:
      - saaster-network
    stdin_open: true
    healthcheck:
      test: ["CMD", "tctl", "--address", "temporal:7233", "namespace", "list"]
      interval: 5s
      timeout: 5s
      retries: 5
    tty: true
  temporal-ui:
    container_name: temporal-ui
    depends_on:
      temporal:
        condition: service_healthy
    environment:
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_CORS_ORIGINS=http://localhost:3000
    image: temporalio/ui:${TEMPORAL_UI_VERSION}
    networks:
      - saaster-network
    ports:
      - "8081:8080"


  # IAM
  postgres:
    image: postgres:15.6
    container_name: postgres_keycloak
    volumes:
      - postgres_data:/var/lib/postgresql/data
    environment:
      POSTGRES_DB: keycloak
      POSTGRES_USER: keycloak
      POSTGRES_PASSWORD: password
    networks:
      - keycloak-network

  keycloak:
    image: quay.io/keycloak/keycloak:latest
    volumes:
      - ./infra/keycloak/imports:/opt/keycloak/data/import
    environment:
      KC_DB: postgres
      KC_DB_URL: jdbc:postgresql://postgres:5432/keycloak
      KC_DB_USERNAME: keycloak
      KC_DB_PASSWORD: password

      KC_HOSTNAME: localhost
      KC_HOSTNAME_STRICT: false
      KC_HOSTNAME_STRICT_HTTPS: false

      KC_LOG_LEVEL: info
      KC_METRICS_ENABLED: true
      KC_HEALTH_ENABLED: true
      KEYCLOAK_ADMIN: admin
      KEYCLOAK_ADMIN_PASSWORD: admin
    command: start-dev --import-realm --verbose
    depends_on:
      - postgres
    ports:
      - "8080:8080"
    networks:
      - saaster-network
      - keycloak-network

  # User Manager Microservice
  user_db:
    image: postgres:${POSTGRESQL_VERSION}
    container_name: user_db
    environment:
      POSTGRES_DB: user_db
      POSTGRES_USER: user_manager
      POSTGRES_PASSWORD: password
    volumes:
      - user_db_data:/var/lib/postgresql/data
    networks:
      - saaster-network
      - user-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U user_manager -d user_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  user_manager:
    build:
      context: ./backend/user_manager
      dockerfile: Dockerfile
    container_name: user_manager
    depends_on:
      user_db:
        condition: service_healthy
      temporal:
        condition: service_healthy
    environment:
      - SERVER_PORT=8080
      - DB_HOST=user_db
      - DB_PORT=5432
      - DB_USER=user_manager
      - DB_PASSWORD=password
      - DB_NAME=user_db
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_NAMESPACE=default
      - TEMPORAL_TASK_QUEUE=user-manager-task-queue
      - KEYCLOAK_URL=http://keycloak:8080
    networks:
      - saaster-network
      - user-network
    ports:
      - "8082:8080"

  user_manager_dapr:
    image: daprio/daprd:1.12.0
    container_name: user_manager_dapr
    depends_on:
      - user_manager
    command: [
      "./daprd",
      "--app-id", "user-manager",
      "--app-port", "8080",
      "--dapr-http-port", "3500",
      "--dapr-grpc-port", "50001",
      "--components-path", "/components",
      "--config", "/config/config.yaml"
    ]
    volumes:
      - ./backend/user_manager/deployments/dapr/components:/components
      - ./backend/user_manager/deployments/dapr:/config
    network_mode: "service:user_manager"

  client_manager_db:
    image: postgres:${POSTGRESQL_VERSION}
    container_name: client_manager_db
    environment:
      POSTGRES_DB: client_manager_db
      POSTGRES_USER: client_manager
      POSTGRES_PASSWORD: password
    volumes:
      - client_manager_db_data:/var/lib/postgresql/data
    networks:
      - saaster-network
      - client-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U client_manager -d client_manager_db"]
      interval: 10s
      timeout: 5s
      retries: 5

  client_manager:
    build:
      context: ./backend/client_manager
      dockerfile: Dockerfile
    container_name: client_manager
    depends_on:
      client_manager_db:
        condition: service_healthy
      temporal:
        condition: service_healthy
    volumes:
      - ./backend/client_manager/scripts:/app/scripts
    environment:
      - SERVER_PORT=8080
      - DB_HOST=client_manager_db
      - DB_PORT=5432
      - DB_USER=client_manager
      - DB_PASSWORD=password
      - DB_NAME=client_manager_db
      - TEMPORAL_ADDRESS=temporal:7233
      - TEMPORAL_NAMESPACE=client-namespace
      - TEMPORAL_TASK_QUEUE=client-manager-task-queue
      - KEYCLOAK_URL=http://keycloak:8080
    networks:
      - saaster-network
      - client-network
    ports:
      - "8083:8080"

  client_manager_dapr:
    image: daprio/daprd:1.12.0
    container_name: client_manager_dapr
    depends_on:
      - client_manager
    command: [
      "./daprd",
      "--app-id", "client-manager",
      "--app-port", "8080",
      "--dapr-http-port", "3500",
      "--dapr-grpc-port", "50001",
      "--components-path", "/components",
      "--config", "/config/config.yaml"
    ]
    volumes:
      - ./backend/client_manager/deployments/dapr/components:/components
      - ./backend/client_manager/deployments/dapr/config:/config
    network_mode: "service:client_manager"

networks:
  saaster-network:
    driver: bridge
    name: saaster-network
  temporal-network:
    driver: bridge
    name: temporal-network
  keycloak-network:
    driver: bridge
    name: keycloak-network
  user-network:
    driver: bridge
    name: user-network
  client-network:
    driver: bridge
    name: client-network

volumes:
  postgres_data:
  user_db_data:
  client_manager_db_data:
  temporal_postgres_data:
  elasticsearch_data:
  prometheus_data:
  grafana_data:
